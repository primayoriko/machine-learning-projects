# -*- coding: utf-8 -*-
"""word2vec_rnn.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1b42vVC7kXjbuhHRcIDI0GUCgXy4JFJbm
"""

!wget --no-check-certificate https://dumps.wikimedia.org/idwiki/latest/idwiki-latest-pages-articles.xml.bz2 -O ./idwiki-latest-pages-articles.xml.bz2

!wget --no-check-certificate https://raw.githubusercontent.com/taufiqhusada/NLP_Class_Works/main/data_worthcheck/test.csv?token=ALOWBURT7X74SJ3IWYGH6RDBLA4IY -O ./test.csv
!wget --no-check-certificate https://raw.githubusercontent.com/taufiqhusada/NLP_Class_Works/main/data_worthcheck/train.csv?token=ALOWBUX542L7IOALRNKR42LBLBUJA -O ./train.csv
!wget --no-check-certificate https://raw.githubusercontent.com/taufiqhusada/NLP_Class_Works/main/data_worthcheck/dev.csv?token=ALOWBUWGBX44TGQAO34UEBTBLBUIS -O ./dev.csv

!wget --no-check-certificate https://raw.githubusercontent.com/haryoa/indo-collex/main/dict/inforformal-formal-Indonesian-dictionary.tsv -O ./inforformal-formal-Indonesian-dictionary.tsv

!ls

import logging
import os.path
import sys
import warnings

def setup_logger():
  warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')
  logging.basicConfig(format='%(asctime)s: %(levelname)s: %(message)s')
  logging.root.setLevel(level=logging.INFO)
  program = os.path.basename(sys.argv[0])
  logger = logging.getLogger(program)
  logger.info("running %s" % ' '.join(sys.argv))

  return logger
  
logger = setup_logger()

from gensim.corpora import WikiCorpus
 

def create_corpus(input_file_name, output_file_name):
  output = open(output_file_name, 'w', encoding='utf-8')
  
  wiki = WikiCorpus(input_file_name, lemmatize=False, dictionary={}, lower=False)
  
  i = 0
  for text in wiki.get_texts():
      output.write(' '.join(text) + '\n')
      i = i + 1
      if i % 10000 == 0:
          logger.info(f"Saved {i} articles")
  
  output.close()
  logger.info(f"Finished Saved {i} articles")

input_file_name = "idwiki-latest-pages-articles.xml.bz2"
output_file_name = "id.text" 

# create_corpus(input_file_name=input_file_name, output_file_name=output_file_name)

import multiprocessing
from gensim.models import Word2Vec
from gensim.models.word2vec import LineSentence
 
input_file_name = "id.text"
output_file_name = "w2vec_wiki_id48_0.txt"
keyed_vector_output_file_name = "keyedvector_w2vec_wiki_id48_0.txt"
 
# model = Word2Vec(LineSentence(input_file_name), size=48, window=4, min_count=4, sg=0, workers=multiprocessing.cpu_count())
 
# model.init_sims(replace=True)
# model.wv.save_word2vec_format(output_file_name, binary=False)
# model.wv.save(keyed_vector_output_file_name)

# model = gensim.models.Word2Vec.load(output_file_name)

class CustomDataset():
    LABEL2INDEX = {'no': 0, 'yes': 1,}
    INDEX2LABEL = {0: 'no', 1: 'yes'}
    NUM_LABELS = 2
    
    def __init__(self, dataset_path, *args, **kwargs):
        df = pd.read_csv(dataset_path)
        df['label'] = df['label'].apply(lambda lab: self.LABEL2INDEX[lab])
        self.data = df
    
    def __getitem__(self, index):
        data = self.data.loc[index,:]
        return data['text_a'], data['label']
        # subwords = self.tokenizer.encode(text)
        # return np.array(subwords), np.array(label), data['text_a']
    
    def __len__(self):
        return len(self.data)  

    def get_labels(self):
      return self.data['label']

    def get_texts(self):
      return self.data['text_a']
      
    def apply_to_data(self, transform_func):
      self.data['text_a'] = self.data['text_a'].apply(transform_func)

import numpy as np
from gensim.models import KeyedVectors

keyed_vector_model = KeyedVectors.load(keyed_vector_output_file_name)

train_dataset_path = 'train.csv'
test_dataset_path = 'test.csv'
dev_dataset_path = 'dev.csv'

import pandas as pd

train_dataset = CustomDataset(train_dataset_path, lowercase=True)
test_dataset = CustomDataset(test_dataset_path, lowercase=True)
dev_dataset = CustomDataset(dev_dataset_path, lowercase=True)

train_dataset.data.head()

dev_dataset.data.head()

test_dataset.data.head()

train_dataset.data.describe()

dev_dataset.data.describe()

test_dataset.data.describe()

train_dataset.data.shape

dev_dataset.data.shape

test_dataset.data.shape

import pandas as pd

df = pd.read_csv('inforformal-formal-Indonesian-dictionary.tsv', sep='\t')
dict_informal_to_formal = {}
for i, row in df.iterrows():
  dict_informal_to_formal[row['informal']] = row['formal']
dict_informal_to_formal

def convert_to_formal_text(s):
  preprocessed_s = ""
  for word in s.split():
    if word in dict_informal_to_formal:
      word = dict_informal_to_formal[word]
    preprocessed_s += word + " "
  return preprocessed_s

def create_embedding_matrix(tokenizer, model, embedding_dim=48):
  vocab_size = len(tokenizer.word_index) + 1
  embedding_matrix = np.zeros((vocab_size, embedding_dim))
  for word, i in tokenizer.word_index.items():
    if word in model.wv.vocab: 
      embedding_matrix[i] = model[word]
  return embedding_matrix

train_dataset.apply_to_data(convert_to_formal_text)
dev_dataset.apply_to_data(convert_to_formal_text)
test_dataset.apply_to_data(convert_to_formal_text)

from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.preprocessing.text import Tokenizer

all_texts = texts = pd.concat([
                               train_dataset.get_texts(), 
                               dev_dataset.get_texts(), 
                               test_dataset.get_texts()
                               ], 
                              axis=0, ignore_index=True).astype("str")
tokenizer = Tokenizer()
tokenizer.fit_on_texts(all_texts)
embedding_matrix = create_embedding_matrix(tokenizer, keyed_vector_model)

padded_length = 100

seq = tokenizer.texts_to_sequences(train_dataset.get_texts())
X_train = pad_sequences(seq, maxlen=padded_length, padding='post', truncating='post')
y_train = train_dataset.get_labels()

seq = tokenizer.texts_to_sequences(dev_dataset.get_texts())
X_val = pad_sequences(seq, maxlen=padded_length, padding='post', truncating='post')
y_val = dev_dataset.get_labels()

seq = tokenizer.texts_to_sequences(test_dataset.get_texts())
X_test = pad_sequences(seq, maxlen=padded_length, padding='post', truncating='post')
y_test = test_dataset.get_labels()

import tensorflow as tf

from tensorflow.keras.models import Sequential
# from keras.models import Model
from tensorflow.keras.layers import Dense, Flatten, LSTM, Dropout, Activation, Embedding, Bidirectional
# from keras.layers.normalization import BatchNormalization
# from keras.layers.merge import concatenate
# from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation

vocab_size = len(tokenizer.word_index) + 1
padded_length = 100
corpus_vector_dim = 48

rnn_model = Sequential([
    Embedding(input_dim=vocab_size, output_dim=corpus_vector_dim, weights=[embedding_matrix], input_length=padded_length, trainable=False),
    Bidirectional(LSTM(64)),
    # tf.keras.layers.LSTM(64),
    Dense(64, activation='relu'),
    Dense(32, activation='relu'),
    Dense(16, activation='relu'),
    Dense(1, activation='sigmoid')
])

rnn_model.summary()

from keras import backend as K

def recall_m(y_true, y_pred):
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))
    recall = true_positives / (possible_positives + K.epsilon())
    return recall

def precision_m(y_true, y_pred):
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))
    precision = true_positives / (predicted_positives + K.epsilon())
    return precision

def f1_m(y_true, y_pred):
    precision = precision_m(y_true, y_pred)
    recall = recall_m(y_true, y_pred)
    return 2*((precision*recall)/(precision+recall+K.epsilon()))

opt = tf.keras.optimizers.Adam(learning_rate=0.001, decay=1e-6)

rnn_model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy', recall_m, precision_m, f1_m])

from keras.callbacks import EarlyStopping, ReduceLROnPlateau
# from keras.callbacks import EarlyStopping, ModelCheckpoint

num_epochs = 15
history_callback = rnn_model.fit(
        X_train, y_train, 
        epochs=num_epochs, verbose=2,
        validation_data=(X_val, y_val), 
        callbacks=[
           EarlyStopping(patience=4, restore_best_weights=True),
           ReduceLROnPlateau(patience=3)
        ])

import matplotlib.pyplot as plt

def plot_graphs(history, string):
  plt.plot(history.history[string])
  plt.plot(history.history['val_'+string])
  plt.xlabel("Epochs")
  plt.ylabel(string)
  plt.legend([string, 'val_'+string])
  plt.show()
  
plot_graphs(history_callback, "accuracy")
plot_graphs(history_callback, "precision_m")
plot_graphs(history_callback, "recall_m")
plot_graphs(history_callback, "f1_m")
plot_graphs(history_callback, "loss")

from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score

y_pred = rnn_model.predict(X_test)
y_pred = [1 if y >= 0.5 else 0 for y in y_pred]

# print(y_pred)

acc = accuracy_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred, average='macro')
rec = recall_score(y_test, y_pred, average='macro')
prec = precision_score(y_test, y_pred, average='macro')

print("Acc: ", acc)
print("F1: ", f1)
print("recall: ", rec)
print("precision: ", prec)