# -*- coding: utf-8 -*-
"""bbc-news-classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qWrjpMoEXRkmVYwsQteEr-UWW5O6NMrE

# **BBC News Classification**

-------

Name: Naufal Prima Yoriko <br/>
Email: primayoriko@gmail.com <br/>

# **A. Import Data**
"""

# import data
!wget --no-check-certificate https://storage.googleapis.com/dataset-uploader/bbc/bbc-text.csv -O ./bbc-text.csv

!ls

# Load data
import pandas as pd

df = pd.read_csv('bbc-text.csv')

"""# **B. Data Description**"""

df.head()

df.describe()

df.shape

"""# **C. Data Preprocess**

## **1. Filter blank or null text**
"""

filter = df["text"] != ""
df = df[filter]
df = df.dropna()

df.shape

X = df["text"]
y = df["category"]

"""## **2. Lowering case of the text**"""

X.apply(lambda el: el.lower())

"""## **3. Tokenize with punctuations removal of the text**"""

from nltk.tokenize import RegexpTokenizer

tokenizer = RegexpTokenizer(r'\w+')
X.apply(lambda el: tokenizer.tokenize(el))

"""## **4. Remove all stop words**"""

import nltk
nltk.download('stopwords')
nltk.download('punkt')

from nltk.corpus import stopwords 
  
stop_words = set(stopwords.words('english')) 

def remove_stop_words(words, stop_words):
  filtered_sentence = []
  for w in words:
      if w not in stop_words:
          filtered_sentence.append(w)
  return filtered_sentence

X.apply(lambda el: remove_stop_words(el, stop_words))

X.head()

"""## **5. Lemmatize word in the text**"""

import nltk
nltk.download('wordnet')

from nltk.stem import WordNetLemmatizer

wordnet_lemmatizer = WordNetLemmatizer()

def lemmatize(words):
  lemmatized_sentence = []
 
  for word in words:
    lemmatized_sentence.append(wordnet_lemmatizer.lemmatize(word))

  return lemmatized_sentence

X.apply(lambda el: lemmatize(el))

X.head()

"""# **D. Process data**

## **1. Convert NLTK token with sklearn token**
"""

import numpy as np

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

X.apply(lambda el: " ".join(el))

X = X.values

X = np.array(X, dtype='O')
  
tokenizer = Tokenizer(num_words=5000, oov_token='oov')
tokenizer.fit_on_texts(X) 
  
X = tokenizer.texts_to_sequences(X)
X = pad_sequences(X, maxlen=600, padding='post', truncating='post')

print(X[0])

# y = pd.get_dummies(y)
y = np.array(pd.get_dummies(y).values, dtype='O')

# Split data
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=63)

print(X[0])
print(y[0])

X_train_tensor = tf.convert_to_tensor(
    X_train, dtype=tf.int32
)

X_test_tensor = tf.convert_to_tensor(
    X_test, dtype=tf.int32
)

y_train_tensor = tf.convert_to_tensor(
    y_train, dtype=tf.bool
)

y_test_tensor = tf.convert_to_tensor(
    y_test, dtype=tf.bool
)

"""## **2. Create ANN model**"""

import tensorflow as tf

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten, LSTM, Dropout, Activation, Embedding, Bidirectional

model = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=5000, output_dim=16),
    tf.keras.layers.Bidirectional(LSTM(64)),
    # tf.keras.layers.LSTM(64),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(5, activation='softmax')
])

model.summary()

opt = tf.keras.optimizers.Adam(learning_rate=0.001, decay=1e-6)

model.compile(loss='categorical_crossentropy',optimizer=opt,metrics=['accuracy'])

"""## **3. Train model with data**"""

num_epochs = 15
history = model.fit(X_train_tensor, y_train_tensor, epochs=num_epochs, 
                    validation_data=(X_test_tensor, y_test_tensor), verbose=2)

# try with callback 
from keras.callbacks import EarlyStopping, ReduceLROnPlateau

num_epochs = 12
history_callback = model.fit(
        X_train_tensor, y_train_tensor, 
        epochs=num_epochs, verbose=2,
        validation_data=(X_test_tensor, y_test_tensor), 
        callbacks=[
           EarlyStopping(patience=4, restore_best_weights=True),
           ReduceLROnPlateau(patience=3)
        ])

"""## **4. Create graph of training process**

### **a. Without callback**
"""

import matplotlib.pyplot as plt

def plot_graphs(history, string):
  plt.plot(history.history[string])
  plt.plot(history.history['val_'+string])
  plt.xlabel("Epochs")
  plt.ylabel(string)
  plt.legend([string, 'val_'+string])
  plt.show()
  
plot_graphs(history, "accuracy")
plot_graphs(history, "loss")

"""### **b. With callback**"""

import matplotlib.pyplot as plt

def plot_graphs(history, string):
  plt.plot(history.history[string])
  plt.plot(history.history['val_'+string])
  plt.xlabel("Epochs")
  plt.ylabel(string)
  plt.legend([string, 'val_'+string])
  plt.show()
  
plot_graphs(history_callback, "accuracy")
plot_graphs(history_callback, "loss")