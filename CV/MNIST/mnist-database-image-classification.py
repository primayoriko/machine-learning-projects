# -*- coding: utf-8 -*-
"""mnist-database-image-classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jWY2rf6ui4MX0A08C1IKzaQigNMTLQH1

# **MNIST Database Image Classification with Deploy Training**

-------

**Name:** Naufal Prima Yoriko <br/>
**Email:** primayoriko@gmail.com <br/>

# **A. Prepare Data**

## **1. Import Data**

using MNIST data from keras datasets
"""

from keras.datasets import mnist

"""## **2. Load Data**

load the data into the test and train set, but in this notebook only test will be used.
"""

(train_X, train_y), (test_X, test_y) = mnist.load_data()

"""# **B. Explore Data**

See the dimension of dataset
"""

print('Train: X=%s, y=%s' % (train_X.shape, train_y.shape))
print('Test: X=%s, y=%s' % (test_X.shape, test_y.shape))

"""Try to see some samples"""

from matplotlib import pyplot

pyplot.imshow(train_X[122], cmap=pyplot.get_cmap('gray'))
pyplot.show()

print(train_y[122])
print(train_X[122])

pyplot.imshow(train_X[30], cmap=pyplot.get_cmap('gray'))
pyplot.show()

print(train_y[30])
print(train_X[30])

pyplot.imshow(test_X[1220], cmap=pyplot.get_cmap('gray'))
pyplot.show()

print(test_y[1220])
print(test_X[1220])

"""# **C. Preprocess Data**

## **1. Adjust set size**

From the specification, its spedified train:test = 8:2,  so it is must be 56000:14000, 4000 data on train must be moved.
"""

import numpy as np

[a, b] = np.split(train_X, [56000])
test_X = np.concatenate((b, test_X))
train_X = a

[a, b] = np.split(train_y, [56000])
test_y = np.concatenate((b, test_y))
train_y = a

print('Train: X=%s, y=%s' % (train_X.shape, train_y.shape))
print('Test: X=%s, y=%s' % (test_X.shape, test_y.shape))

"""## **2. Change dimension**

the feature array (X) should be in 4-dim to be processed in CNN by keras API, n now its only 3-dim. So the ndarray should be reshaped.
"""

train_X = train_X.reshape(train_X.shape[0], 28, 28, 1)
test_X = test_X.reshape(test_X.shape[0], 28, 28, 1)

print('Train: X=%s, y=%s' % (train_X.shape, train_y.shape))
print('Test: X=%s, y=%s' % (test_X.shape, test_y.shape))

"""## **3. Normalize the value**

the value in the pixel is now in scale of 0 - 255, to get better result, we could normalize it in the scale of 0 - 1.
"""

train_X = train_X.astype('float32')
test_X = test_X.astype('float32')

train_X /= 255.0
test_X /= 255.0

"""# **D. Process Data**

## **1. Create model**
"""

import tensorflow as tf

input_shape = (28, 28, 1)

model = tf.keras.models.Sequential([
  tf.keras.layers.Conv2D(28, kernel_size=(3,3), input_shape=input_shape),
  tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),
  tf.keras.layers.Flatten(),
  tf.keras.layers.Dense(128, activation='relu'),
  tf.keras.layers.Dense(10,activation='softmax'),
])

loss = 'sparse_categorical_crossentropy'
optimizer = 'adam'
metrics = ['accuracy']

model.compile(loss=loss,
              optimizer=optimizer,
              metrics=metrics)

"""## **2. Train Data**"""

from keras.callbacks import EarlyStopping, ReduceLROnPlateau

n_batchs = 30
n_epochs = 12 

keras_dict = {
      'x': train_X,
      'y': train_y,
      'validation_data': (test_X, test_y),
      'batch_size': n_batchs,
      'epochs': n_epochs,
      'shuffle': False,
      'callbacks': [
           EarlyStopping(patience=5, restore_best_weights=True),
           ReduceLROnPlateau(patience=4)
      ]
  }

history = model.fit(**keras_dict)

"""## **3. Visualize Result**

by create graph of it's metrics
"""

import matplotlib.pyplot as plt

def plot_graphs(history, string):
  plt.plot(history.history[string])
  plt.plot(history.history['val_'+string])
  plt.xlabel("Epochs")
  plt.ylabel(string)
  plt.legend([string, 'val_'+string])
  plt.show()

for val in metrics:
  plot_graphs(history, val)

plot_graphs(history, "loss")

"""# **D. Process Data**

By export with TF-Lite
"""

converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_model = converter.convert()

with tf.io.gfile.GFile('model.tflite', 'wb') as f:
  f.write(tflite_model)

!cat model.tflite