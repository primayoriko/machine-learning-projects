# -*- coding: utf-8 -*-
"""pjm-energy-consumption-prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wlXZjJVLqeEiGU3nRlyg5pJX0f4dcUlK

# **PJM Energy Consumption Prediction**

-------

**Name:** Naufal Prima Yoriko <br/>
**Email:** primayoriko@gmail.com <br/>

# **A. Prepare Data**

## **1. Import Data**

using Kaggle API
"""

# install kaggle api program 
!pip install --upgrade --force-reinstall --no-deps kaggle

"""please import your kaggle api keys"""

from google.colab import files 
files.upload()

"""this is kaggle api key example of mine <br/>
`
{"username":"primayoriko","key":"580***ff5f3c6c55bc6***8424**7c80"}
`
<br/>
make your own kaggle api key, if dont know, see the tutorial post [here](https://www.kaggle.com/general/74235).

"""

!mkdir ~/.kaggle 
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json
print("finished prepare api key file!")

# test kaggle api program 
 !kaggle datasets list

"""download the dataset <br/>
**NOTE:**  make sure you have accept terms in the kaggle site, in case needed (some data need it) <br/>
https://www.kaggle.com/robikscube/hourly-energy-consumption
"""

!kaggle datasets download -d robikscube/hourly-energy-consumption --force

!ls

# Extract file
import zipfile,os

local_zip = './hourly-energy-consumption.zip'
zip_ref = zipfile.ZipFile(local_zip, 'r')
zip_ref.extractall('./')
zip_ref.close()

!ls

"""## **2. Load Data**

here we're just using data from DEOK `DEOK_hourly.csv` for this practice
"""

import pandas as pd

df = pd.read_csv('DEOK_hourly.csv')

"""# **B. Explore Data**"""

df.head()

df.shape

df.isnull().sum()

import matplotlib.pyplot as plt

datetimes = df['Datetime'].values[0:2000]
energy_usages  = df['DEOK_MW'].values[0:2000]
  
plt.figure(figsize=(20,5))
plt.plot(datetimes, energy_usages)
plt.title('Energy Consumption',
          fontsize=20);

"""# **C. Preprocess Data**

## **1. Make preprocess helper function**

### **1.1. X y helper function**

Help to create X with appropriate window size and y with appropriate feature size.
"""

import numpy as np

def create_X_y(ts: list, window_size: int, feature_size: int = 1) -> tuple:
  X, y = [], []

  if len(ts) - window_size <= 0:
      X.append(ts)
  else:
      for i in range(len(ts) - window_size):
          y.append(ts[i + window_size])
          X.append(ts[i:(i + window_size)])

  X, y = np.array(X), np.array(y)

  # Reshaping the X array to an LSTM input shape 
  X = np.reshape(X, (X.shape[0], X.shape[1], feature_size))

  return X, y

"""### **1.1. train test split helper function**

help to split data with specified ratio
"""

def split_data(X: list, y: list, train_test_split: int) -> tuple:
  X_train = X
  X_test = []

  y_train = y
  y_test = []

  index = round(len(X) * train_test_split)
  X_train = X[:(len(X) - index)]
  X_test = X[-index:]     

  y_train = y[:(len(X) - index)]
  y_test = y[-index:]

  return X_train, X_test, y_train, y_test

"""## **2. Declare constants**

probably needed in the process phase, too.
"""

n_features = 1
n_windows = 80
n_batchs = 150
n_epochs = 25
# n_batch = math.floor(len(y)/n_windows)

train_test_ratio = 0.8

"""## **3. Do preprocess pipeline**

### **3.1. drop null data**
"""

df = df.dropna()

"""## **4. Split data**"""

# from sklearn.model_selection import train_test_split

# axis_value = df['Datetime'].values
feature_value = df['DEOK_MW'].values

X, y = create_X_y(ts=feature_value, window_size=n_windows, feature_size=n_features)

# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
X_train, X_test, y_train, y_test = split_data(X, y, train_test_split=train_test_ratio)

train_set = windowed_dataset(y, window_size=n_windows, batch_size=n_batch, shuffle_buffer=1000)

"""# **D. Process Data**

## **1. Create model**
"""

import tensorflow as tf

model = tf.keras.models.Sequential([
  tf.keras.layers.LSTM(60, return_sequences=True),
  tf.keras.layers.LSTM(60),
  tf.keras.layers.Dense(30, activation="relu"),
  tf.keras.layers.Dense(10, activation="relu"),
  tf.keras.layers.Dense(1),
])

optimizer = tf.keras.optimizers.SGD(learning_rate=1.0000e-04, momentum=0.9)
model.compile(loss=tf.keras.losses.Huber(),
              optimizer=optimizer,
              metrics=["mae"])

"""## **2. Train Data**"""

from keras.callbacks import EarlyStopping, ReduceLROnPlateau

keras_dict = {
      'x': X_train,
      'y': y_train,
      'validation_data': (X_test, y_test),
      'batch_size': n_batchs,
      'epochs': n_epochs,
      'shuffle': False,
      'callbacks': [
           EarlyStopping(patience=10, restore_best_weights=True),
           ReduceLROnPlateau(patience=8)
      ]
  }

history = model.fit(**keras_dict)

"""## **3. Visualize Result**

by create graph of it's MAE and loss 
"""

import matplotlib.pyplot as plt

def plot_graphs(history, string):
  plt.plot(history.history[string])
  plt.plot(history.history['val_'+string])
  plt.xlabel("Epochs")
  plt.ylabel(string)
  plt.legend([string, 'val_'+string])
  plt.show()
  
plot_graphs(history, "mae")
plot_graphs(history, "loss")

"""here is relative MAE value to total data

**A. Train relative MAE**
"""

print((462.8622/(df.shape[0] * train_test_ratio)) * 100, "%")

"""**B. Test relative MAE**"""

print((471.8848/(df.shape[0] * (1 - train_test_ratio))) * 100, "%")

"""# **E. Attachment**

## **1. Old result with 100 epoch**
"""

import tensorflow as tf

model = tf.keras.models.Sequential([
  tf.keras.layers.LSTM(60, return_sequences=True),
  tf.keras.layers.LSTM(60),
  tf.keras.layers.Dense(30, activation="relu"),
  tf.keras.layers.Dense(10, activation="relu"),
  tf.keras.layers.Dense(1),
])
optimizer = tf.keras.optimizers.SGD(learning_rate=1.0000e-04, momentum=0.9)
model.compile(loss=tf.keras.losses.Huber(),
              optimizer=optimizer,
              metrics=["mae"])
history = model.fit(train_set,epochs=100)